{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["cxys4s5y3jyB","SZDPurkd6o4c","hQcdCSSF-S4g","koBSDe_0NcNA","rWPao0rkzy9H","82_qX7Ib3PUA","4f3sJuGtRP6P"],"authorship_tag":"ABX9TyNbbnwl3buGyv5pzvs0bj/o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Question 1: Findiing the descriptive statistics for temperature of each day of a given month for the year 2007."],"metadata":{"id":"BR0UMhuM1YDZ"}},{"cell_type":"code","source":[],"metadata":{"id":"hAPuvdmbeJTv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For each of the following, name a file using ***nano filename_________*** then input the code into each pop up window. Save and exit using Cntrl + x. Press Y/N after renaming. Then press Enter to exit."],"metadata":{"id":"XAHr2vOJd9mv"}},{"cell_type":"markdown","source":["##Mapper 1\n","\n","Reads the input line by line from sys.stdin.  \n","Split each line by commas into fields.  \n","Checks if the line has at least 12 fields.  \n","If not, skip the line.   \n","\n","Extract the date (YearMonthDay) and wind speed.  \n","Try to convert wind speed to a float. If conversion fails, skip the line.\n","\n","Print output in the format: YearMonthDay _________ WindSpeed"],"metadata":{"id":"cxys4s5y3jyB"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Kfr-_tVu1MZH","executionInfo":{"status":"ok","timestamp":1741672088238,"user_tz":0,"elapsed":17,"user":{"displayName":"Oluchi Otuadinma","userId":"13554560614300017259"}}},"outputs":[],"source":["#Mapper 1\n","import sys\n","\n","for line in sys.stdin:\n","    fields = line.strip().split(\",\")\n","\n","    if len(fields) < 12:\n","        continue  # Skip lines with missing data\n","\n","    try:\n","        date = fields[1]  # YearMonthDay\n","        wind_speed = float(fields[12])  # % Wind Speed\n","        print(f\"{date}\\t{wind_speed}\")\n","    except ValueError:\n","        continue  # Skip invalid lines"]},{"cell_type":"markdown","source":["##Reducer 1\n","\n","Initialize the variables.    \n","\n","current_day = None  \n","max_wind_speed = -infinity (the smallest possible value)  \n","min_wind_speed = +infinity (the largest possible value)  \n","                                                         \n","Read input line by line from sys.stdin.  \n","Split each line into day and wind_speed.  \n","Try to convert wind_speed to float.\n","If conversion fails, skip the line.  \n","\n","Check if the day is the same as current_day.  \n","Yes → Update max_wind_speed and min_wind_speed\n","\n","No →\n","If current_day is set,   \n","Print:\n","\"current_day _________ (max_wind_speed - min_wind_speed)\"\n","\n","Reset values for the new day\n","After the loop ends.  \n","Print the last day's result"],"metadata":{"id":"SZDPurkd6o4c"}},{"cell_type":"code","source":["#Reducer 1\n","\n","import sys\n","\n","current_day = None\n","max_wind_speed = float(\"-inf\")\n","min_wind_speed = float(\"inf\")\n","\n","for line in sys.stdin:\n","    # Read input: Expected format -> YearMonthDay \\t WindSpeed\n","    line = line.strip()\n","    try:\n","        day, wind_speed = line.split(\"\\t\")\n","        wind_speed = float(wind_speed)\n","    except ValueError:\n","        continue  # Skip malformed lines\n","\n","    # If we are still processing the same day\n","    if current_day == day:\n","        max_wind_speed = max(max_wind_speed, wind_speed)\n","        min_wind_speed = min(min_wind_speed, wind_speed)\n","    else:\n","        # Print result for the previous day\n","        if current_day:\n","            print(f\"{current_day}\\t{max_wind_speed - min_wind_speed}\")\n","\n"],"metadata":{"id":"b6ncfJuj1ce0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Mapper 2\n","This Mapper does similar to the last but instead it:   \n","Extracts the date (YearMonthDay) and relative humidity.  \n","Tries to convert relative humidity to a float.  \n","If the conversion fails, skips the line  \n","Prints the output in the format: YearMonthDay _________ RelativeHumidity"],"metadata":{"id":"hQcdCSSF-S4g"}},{"cell_type":"code","source":["#Mapper 2\n","\n","import sys\n","\n","for line in sys.stdin:\n","    fields = line.strip().split(\",\")\n","\n","    if len(fields) < 12:\n","        continue  # Skip lines with missing data\n","\n","    try:\n","        date = fields[1]  # YearMonthDay\n","        relative_humidity = float(fields[11])  # % Relative Humidity\n","        print(f\"{date}\\t{relative_humidity}\")\n","    except ValueError:\n","        continue  # Skip invalid lines"],"metadata":{"id":"H0Us0orc1iEh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Reducer 2\n","\n","Initialize variables.\n","\n","current_date = None\n","\n","min_relative_humidity = +infinity\n","\n","\n","Parse through each line splitting commas into fields.    \n","\n","Remove whitespace and split line into date and relative_humidity.   Try to convert relative_humidity to float.  \n","If conversion fails, skip the line.  \n","Check if date is the same as current_date  \n","Yes → Update min_relative_humidity  \n","No →\n","If current_date is set, print result  \n","\n","current_date _________ min_relative_humidity\n","Reset variables for the new date  \n","After loop ends, print last date’s result"],"metadata":{"id":"koBSDe_0NcNA"}},{"cell_type":"code","source":["#Reducer 2\n","\n","import sys\n","\n","current_date = None\n","min_relative_humidity = float('inf')\n","\n","for line in sys.stdin:\n","    line = line.strip()\n","    date, relative_humidity = line.split(\"\\t\")\n","\n","    try:\n","        relative_humidity = float(relative_humidity)\n","    except ValueError:\n","        continue  # Skip invalid values\n","\n","    if date == current_date:\n","        # Keep track of the minimum relative humidity\n","        min_relative_humidity = min(min_relative_humidity, relative_humid)\n","    else:\n","        if current_date is not None:\n","            # Output the minimum relative humidity for the previous date\n","            print(f\"{current_date}\\t{min_relative_humidity}\")\n","\n","        # Update the current date and reset the minimum relative humidity\n","        current_date = date\n","        min_relative_humidity = relative_humidity\n"],"metadata":{"id":"m5Qd9QVK1lGy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Mapper 3\n","Parse through each line splitting commas into fields.      \n","Checks if the line has at least 10 fields.  \n","If not, skip the line.   \n","Extract the date (YearMonthDay) and dew point temperature.   \n","Try to convert dew point temperature to a float\n","If conversion fails, skip the line.   \n","Print output in the format:   \n","YearMonthDay _________ DewPointTemperature"],"metadata":{"id":"rWPao0rkzy9H"}},{"cell_type":"code","source":["#Mapper 3\n","import sys\n","\n","for line in sys.stdin:\n","    fields = line.strip().split(\",\")\n","\n","    if len(fields) < 10:  # Ensure enough columns\n","        continue\n","\n","    try:\n","        date = fields[1]  # YearMonthDay\n","        dew_point_temp = float(fields[9])  # Dew Point Temp\n","        print(f\"{date}\\t{dew_point_temp}\")\n","    except ValueError:\n","        continue  # Skip invalid data\n"],"metadata":{"id":"9leULRrj1nwy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Reducer 3\n","\n","Reducer 3\n","Initialize variables.  \n","current_date = None  \n","count = 0 (Number of data points)  \n","sum_dew = 0.0 (Sum of dew point temperatures)  \n","sum_sq_dew = 0.0 (Sum of squared dew point temperatures for variance calculation)  \n","\n","Parse through each line splitting commas into fields.     \n","Remove whitespace and split line into date and dew_point_temp.   \n","Try to convert dew_point_temp to float.  \n","If conversion fails, skip the line   \n","Check if date is the same as current_date:\n","\n","Yes → Update count, sum_dew, and sum_sq_dew\n","\n","No →\n","\n","If current_date is set, compute mean and variance:   \n","mean = sum_dew / count   \n","variance = (sum_sq_dew / count) - (mean * mean)\n","\n","Print Result:\n","current_date _________ mean _________ variance   \n","Reset variables for the new date   \n","After loop ends, print last date’s result"],"metadata":{"id":"eaa6RZS8JvYI"}},{"cell_type":"code","source":["#Reducer 3\n","\n","import sys\n","\n","current_date = None\n","count = 0\n","sum_dew = 0.0\n","sum_sq_dew = 0.0  # Sum of squares for variance calculation\n","\n","for line in sys.stdin:\n","    line = line.strip()\n","    date, dew_point_temp = line.split(\"\\t\")\n","\n","    try:\n","        dew_point_temp = float(dew_point_temp)\n","    except ValueError:\n","        continue  # Skip invalid values\n","\n","    if date == current_date:\n","        count += 1\n","        sum_dew += dew_point_temp\n","        sum_sq_dew += dew_point_temp * dew_point_temp\n","    else:\n","        if current_date is not None:\n","            # Compute mean and variance\n","            mean = sum_dew / count\n","            variance = (sum_sq_dew / count) - (mean * mean)  # Variance formula\n","\n","            print(f\"{current_date}\\t{mean}\\t{variance}\")\n","\n","        # Reset values for the new date\n","        current_date = date\n","        count = 1\n","        sum_dew = dew_point_temp\n","        sum_sq_dew = dew_point_temp * dew_point_temp\n","\n","# Print last date's values\n","if current_date is not None:\n","    mean = sum_dew / count\n","    variance = (sum_sq_dew / count) - (mean * mean)\n","    print(f\"{current_date}\\t{mean}\\t{variance}\")\n"],"metadata":{"id":"wsapK61z1p_h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Mapper 4\n","\n","Parse through each line splitting commas into fields  \n","\n","Check if the line has at least 13 fields\n","If not, skip the line\n","Extract year_month (YYYYMM) from the date field\n","Extract relative humidity, wind speed, and dry bulb temperature\n","Try to convert all three values to float\n","If conversion fails, skip the line\n","Print output in the format:"],"metadata":{"id":"82_qX7Ib3PUA"}},{"cell_type":"code","source":["\n","#Mapper 4\n","\n","import sys\n","\n","for line in sys.stdin:\n","    fields = line.strip().split(\",\")\n","\n","    if len(fields) < 13:  # Ensure enough columns\n","        continue\n","\n","    try:\n","        year_month = fields[1][:6]  # Extract YearMonth (YYYYMM)\n","        rel_humidity = float(fields[11])  # % Relative Humidity\n","        wind_speed = float(fields[12])  # Wind Speed (kt)\n","        dry_bulb_temp = float(fields[8])  # Dry Bulb Temp\n","\n","        print(f\"{year_month}\\t{rel_humidity},{wind_speed},{dry_bulb_temp}\")\n","    except ValueError:\n","        continue  # Skip invalid data"],"metadata":{"id":"7L-LOAkI1wxZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Reducer 4\n","\n","Initialize variables:\n","\n","current_month = None (Tracks the current processing month)   \n","data = [] (Stores humidity, wind speed, and dry bulb temperature values)   \n","\n","Define function compute_correlation(data) to calculate the correlation matrix:   \n","\n","If data is empty, return None.  \n","\n","Compute sums, means, variances, covariances, and standard deviations.  \n","Calculate the correlation coefficients between humidity, wind speed, and temperature.\n","Return a 3×3 correlation matrix.  \n","\n","\n","Parse through each line.  \n","Removes whitespaces.  \n","\n","Split each line into year_month and values.  \n","Try to extract humidity, wind_speed, and dry_bulb_temp as floats.  \n","If conversion fails, skip the line.  \n","\n","Check if the year_month is the same as current_month:  \n","\n","Yes → Append (humidity, wind_speed, dry_bulb_temp) to data.  \n","No →\n","If current_month exists and data is not empty:\n","Compute correlation matrix using compute_correlation(data).  \n","\n","Print current_month and the matrix.  \n","Reset current_month and data for the new month.  \n","\n","After finishing the loop, process the last batch of data\n","\n","Compute and print correlation for the last stored month.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"4f3sJuGtRP6P"}},{"cell_type":"code","source":["#Reducer 4\n","\n","import sys\n","\n","current_month = None\n","data = []\n","\n","# Function to compute correlation matrix for given data\n","def compute_correlation(data):\n","    n = len(data)\n","    if n == 0:\n","        return None\n","\n","    # Initialize sums for calculations\n","    sum_x = sum_y = sum_z = 0.0\n","    sum_x2 = sum_y2 = sum_z2 = 0.0\n","    sum_xy = sum_xz = sum_yz = 0.0\n","\n","    # Compute required sums\n","    for x, y, z in data:\n","        sum_x += x\n","        sum_y += y\n","        sum_z += z\n","        sum_x2 += x * x\n","        sum_y2 += y * y\n","        sum_z2 += z * z\n","        sum_xy += x * y\n","        sum_xz += x * z\n","        sum_yz += y * z\n","\n","    # Compute means\n","    mean_x = sum_x / n\n","    mean_y = sum_y / n\n","    mean_z = sum_z / n\n","\n","    # Compute variances\n","    var_x = (sum_x2 / n) - (mean_x * mean_x)\n","    var_y = (sum_y2 / n) - (mean_y * mean_y)\n","    var_z = (sum_z2 / n) - (mean_z * mean_z)\n","\n","    # Compute covariances\n","    cov_xy = (sum_xy / n) - (mean_x * mean_y)\n","    cov_xz = (sum_xz / n) - (mean_x * mean_z)\n","    cov_yz = (sum_yz / n) - (mean_y * mean_z)\n","\n","    # Compute the Standard Deviation\n","    std_x = var_x ** 0.5 if var_x > 0 else 1\n","    std_y = var_y ** 0.5 if var_y > 0 else 1\n","    std_z = var_z ** 0.5 if var_z > 0 else 1\n","\n","    # Compute the Correlation Coefficients\n","    corr_xy = cov_xy / (std_x * std_y)\n","    corr_xz = cov_xz / (std_x * std_z)\n","    corr_yz = cov_yz / (std_y * std_z)\n","\n","    # Returns a 3 by 3 matrix\n","    return [[1, corr_xy, corr_xz], [corr_xy, 1, corr_yz], [corr_xz, corr_yz, 1]]\n","\n","# Reads through each line in the data\n","for line in sys.stdin:\n","    line = line.strip() # Removes line whitespace\n","    year_month, values = line.split(\"\\t\") # Split the month and values\n","\n","    try:\n","        # Parse humidity, wind speed, and dry bulb temperature assuming comma separation\n","        humidity, wind_speed, dry_bulb_temp = map(float, values.split(\",\"))\n","    except ValueError:\n","        continue  # Skip invalid values\n","\n","    if year_month == current_month:\n","        data.append([humidity, wind_speed, dry_bulb_temp])\n","    else:\n","        if current_month is not None and data:\n","            correlation_matrix = compute_correlation(data)\n","            if correlation_matrix:\n","                print(f\"{current_month}\")\n","                for row in correlation_matrix:\n","                    print(\"\\t\".join(map(str, row)))\n","                print()\n","\n","        # Reset for new month\n","        current_month = year_month\n","        data = [[humidity, wind_speed, dry_bulb_temp]]\n","\n","# Compute correlation for the last batch\n","if current_month is not None and data:\n","    correlation_matrix = compute_correlation(data)\n","    if correlation_matrix:\n","        print(f\"{current_month}\")\n","        for row in correlation_matrix:\n","            print(\"\\t\".join(map(str, row)))\n","        print()\n"],"metadata":{"id":"y8JRlCuB10jB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Question 2: Cluster Analysis using Apache Mahout"],"metadata":{"id":"_-ENBxOb19Tx"}},{"cell_type":"markdown","source":["The documents are loaded, uncompressed and stored on the cluster. I used the western classics datasets.\n","\n","Sequence files are used as a Hadoop-based format to store data as key-value pairs, representing documents or data points with their associated content or vectors"],"metadata":{"id":"1wj6I74qOq-g"}},{"cell_type":"code","source":["#To create sequence files from the raw text using Mahout\n","mahout seqdirectory -i docs -o docs-seqfiles -c UTF-8 -chunk 5"],"metadata":{"id":"3Pn83_OqOuFL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The sequence files are then converted into a sparse vector representation. These are used for the clustering in Mahout. It takes raw text data and converts it into numerical format (sparse vectors) that can be used for machine learning algorithms. The output is generated using the term frequency (TF) and term frequency/inverse document frequency (TF/IDF) calculations. TF being the number of times a term (word) appears in a document divided by the total number of words in the document.  \n","\n","While TF looks at the importance of a term in a single document. IDF looks at the importance of the term across all the document. its measures how rare or common a term is across the whole collection of documents. Rare words that occur in only a few documents are given higher IDF scores, while common words that appear in many documents have lower IDF scores.\n","\n","IDF is the logarithm of the total number of documents in the corpus divided number of documents containing term. Using the logarithm of the ratio keeps the IDF proportional throughout. TF-IDF is then obtained by multiplying these two numbers together and is used in various fields such as Big Data or Natural Language Processing (NLP)"],"metadata":{"id":"lVBmK2gPjItA"}},{"cell_type":"code","source":["#Creates a sparse (efficient) representation of the vectors using Mahout\n","mahout seq2sparse -nv -i docs-seqfiles -o docs-vectors"],"metadata":{"id":"V615uAtWOvCg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then the runs Canopy Clustering algorithm is done using Apache Mahout. It is a pre-clustering method that provides a rough grouping using cosine distances to determine similarity between documents. The TF-IDF vectors are used group data points into \"canopies\" which are regions that contains a certain number of data points within a specified distance. Overlapping canopies are merged, and the resulting canopies are then used as input for a more refined clustering algorithm.\n","\n","The algorithm is fast and efficient and the resulting output, the canopies, can be used in clustering algorithms reducing the computational time required for large datasets. This makes it great for pre-processing as it speeds up finding the initial cluster centers for K-means clustering.  "],"metadata":{"id":"MAAKvCaGd0No"}},{"cell_type":"code","source":["mahout canopy -i docs-vectors/tfidf-vectors -ow -o docs-vectors/docs-canopy-centroids -dm org.apache.\n","mahout.common.distance.CosineDistanceMeasure -t1 1500 -t2 2000"],"metadata":{"id":"tbWyjz5fOzt5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#To find the ‘final’ iteration’s clustering solution:\n","hadoop fs -ls docs-kmeans-clusters"],"metadata":{"id":"OF2OkFVLbugx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The Mahout K-Means command and clustering results provide insights into how the Cosine Distance Measure affects clustering behavior. Low inter-cluster density and high intra-cluster density indicates a good solution.   \n","I tested different Distance measures."],"metadata":{"id":"L1hgGMBZ6DKP"}},{"cell_type":"code","source":["# Cosine Distance Measure\n","mahout kmeans -i docs-vectors/tfidf-vectors -c docs-canopy-centroids -o docs-kmeans-clusters -dm\n","org.apache.mahout.common.distance.CosineDistanceMeasure -cl -cd 0.1 -ow -x 20 -k 10\n","\n","#K = 1, 0 canopies\n","#Inter-Cluster Density: NaN\n","#Intra-Cluster Density: 0.6603071501779987\n","\n","#k = 3,\n","#Inter-Cluster Density: 0.6406917203066618\n","#Intra-Cluster Density: 0.5808394041592724\n","\n","# k = 2\n","# Inter-Cluster Density: NaN\n","# Intra-Cluster Density: 0.6327216701135798\n","\n","# #k = 4\n","# Inter-Cluster Density: 0.5467877384987859\n","# Intra-Cluster Density: 0.5609998998760057\n","\n","#k = 5\n","#Inter-Cluster Density: 0.5157879781408249\n","#Intra-Cluster Density: 0.6003637963412949\n","\n","# K = 6\n","# Inter-Cluster Density: 0.4879443198926937\n","# Intra-Cluster Density: 0.5697857709312016\n","\n","#K = 7\n","#Inter-Cluster Density: 0.6303194793785727\n","#Intra-Cluster Density: 0.5566106363999812\n","\n","#K = 8\n","# Inter-Cluster Density: 0.5118730800218336\n","# Intra-Cluster Density: 0.5345188295329616\n","\n","# k = 9\n","# Inter-Cluster Density: 0.4210754716419729\n","# Intra-Cluster Density: 0.5736451466731278\n","\n","#K = 10\n","#resulted in 3 canopies\n","#Inter-Cluster Density: 0.3780405811664108\n","#Intra-Cluster Density: 0.6742093298236477\n","\n","#Optimal K value = 5"],"metadata":{"id":"IUSBq1z1O1qg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Euclidean Distance Measure\n","mahout kmeans -i docs-vectors/tfidf-vectors -c docs-canopy-centroids -o docs-kmeans-clusters -dm\n","org.apache.mahout.common.distance.EuclideanDistanceMeasure -cl -cd 0.1 -ow -x 20 -k 10\n","\n","#K = 10\n","#resulted in 21 clusters\n","#Inter-Cluster Density: 0.5387380533065216\n","#Intra-Cluster Density: 0.5424312691336688"],"metadata":{"id":"4gt5xD7boQv-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Manhattan Distance Measure\n","mahout kmeans -i docs-vectors/tfidf-vectors -c docs-canopy-centroids -o docs-kmeans-clusters -dm\n","org.apache.mahout.common.distance.ManhattanDistanceMeasure -cl -cd 0.1 -ow -x 20 -k 10"],"metadata":{"id":"2sarPL2FoREG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Evaluating the output\n","mahout clusterdump -dt sequencefile -d docs-vectors/dictionary.file-* -i docs-kmeans-clusters/clusters-2-final -o clusters.txt -b 100 -p docs-kmeans-clusters/clusteredPoints -n 20 --evaluate"],"metadata":{"id":"4vi5sxWDuaD3"},"execution_count":null,"outputs":[]}]}